# Brain-Tumor-Segmentation

In recent years, the Computer Vision community has made significant strides in medical image segmentation, particularly through the use of convolutional neural networks (CNNs). However, CNN-based methods have limitations in capturing long-range dependencies due to the inherent locality of convolution operations. Inspired by the success of Transformers in natural language processing, we introduce Swin-Unet, a novel model for 2D medical image segmentation that leverages the Swin Transformer. Swin-Unet is the first pure Transformer-based U-shaped architecture, featuring an encoder, bottleneck, decoder, and skip connections built on Swin Transformer blocks. The model processes input images by splitting them into nonoverlapping patches, treating each patch as a token. These tokens are then processed by the Transformer-based encoder to learn deep feature representations. The decoder up-samples these features using a novel patch-expanding layer and integrates multi-scale features from the encoder via skip connections to restore spatial resolution for precise segmentation. Extensive experiments on brain tumor datasets demonstrate that Swin-Unet achieves high accuracy and robust generalization. We used Binary cross-entropy loss to assess the performance of our model, which scored 98.9% accuracy. Swin-Unet represents a significant advancement in leveraging Transformer architectures for medical image segmentation, addressing CNN limitations, and opening new research avenues.
