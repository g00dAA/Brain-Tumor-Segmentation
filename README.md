# Brain-Tumor-Segmentation

In recent years, the Computer Vision community has made significant strides in medical image segmentation, particularly through the use of convolutional neural networks (CNNs). However, CNN-based methods have limitations in capturing long-range dependencies due to the inherent locality of convolution operations. To address this, Transformer-based architectures have been proposed, notably Swin-Unet, a U-shaped model for 2D medical image segmentation that leverages the Swin Transformer. Swin-Unet is a pure Transformer-based architecture comprising an encoder, bottleneck, decoder, and skip connections, all built on Swin Transformer blocks. The model processes input images by splitting them into non-overlapping patches treated as tokens, which are then processed by the encoder to learn deep feature representations. The decoder upsamples these features using a patch-expanding layer and integrates multi-scale information via skip connections to recover spatial resolution for precise segmentation.

In our project, we adopted the Swin-Unet architecture and applied it to brain tumor segmentation tasks. We made several adaptations and tuning strategies to optimize its performance for our specific dataset. We trained the model using binary cross-entropy loss and achieved a segmentation accuracy of 98.9%, demonstrating robust generalization and high precision. While we did not propose the Swin-Unet architecture, our work highlights its effectiveness in brain tumor segmentation and contributes further evidence of its potential in medical imaging applications.


